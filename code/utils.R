messager <- MAGMA.Celltyping:::messager
message_parallel <- MAGMA.Celltyping:::message_parallel


#' Gather metadata for each GWAS dataset
#' 
#' 
#' `MungeSumstats` automatically keeps log files in the `tempdir()`. 
#' But if you restart your R session mid-way, they'll be erased. 
#' Or perhaps you ran `MungeSumstats` without turning on any of the
#'  log arguments.\cr
#' Instead, you can infer the genome build directly from the data. 
#' Note that this will take some time as the genome reference data needed 
#' to make these inferences is quite large.
#' 
#' @param save_dir Directory in which your munged summary statistics
#'  files (and logs) are saved.
#' @param use_logs Whether to search for and extract info from logs generated by 
#' \pkg{MungeSumstats}.
#' @param infer_builds If "inferred_build" is missing from log data 
#' (or there is no log data to search), you can set \code{infer_builds=TRUE} to
#' infer the genome build of each GWAS.
#' @param save_meta Path to save new metadata file to.
#' @param verbose Print messages.
#' 
#' @export
gather_metadata <- function(save_dir, 
                            use_logs = TRUE,
                            infer_builds = FALSE,
                            save_meta = here::here("metadata.csv"),
                            verbose = TRUE){
  #### Search for munged GWAS sumstats ####
  munged_files <- list_sumstats(save_dir = save_dir)
  #### Query OpenGWAS metadata ####
  metagwas_all <- MungeSumstats::find_sumstats(ids = names(munged_files))
  metagwas_all$munged_path <- munged_files[metagwas_all$id]
  #### Add in log data generated by MungeSumstats ####
  if(use_logs){
    log_data <- parse_logs(save_dir = save_dir) 
    #### Merge metadata sources ####
    meta <- merge(metagwas_all, log_data, by = "id")
  }else {meta <- metagwas_all}
  #### Re-infer genome build ####
  if(isTRUE(infer_builds)){
    builds <- MungeSumstats::get_genome_builds(sumstats_list = munged_files, 
                                               header_only = 10000)
    meta$build_inferred <-  builds[meta$id] 
  }
  #### Find which GWAS have different inferred genome builds #### 
  meta <- check_build_matches(meta = meta)
  #### Save metadata ####
  if(!is.null(save_meta)){
    messager("Savning merged metadata ==>",save_meta,v=verbose)
    utils::write.csv(meta,
                     save_meta)
  }
  #### Gather gene mapping files #####
  gene_files <- list_snps_to_genes_files(save_dir = save_dir)
  gene_files$
  return(meta)
}


#' Checks whether OpenGWAS and MungeSumstats genome builds match
#'  
#' Creates a new column to examine
#' whether the genome-build  \link[MungeSumstats]{get_genome_builds}
#' inferred from the each GWAS dataset ("build_inferred") matches the
#' build reported by OpenGWAS ("build").
#' 
#' @param meta GWAS metadata.
#' @param verbose Print messages.
#' 
#' @return GGWAS metadata.
#' 
#' @importFrom stringr str_split
#' @keywords internal
check_build_matches <- function(meta,
                                verbose = TRUE){
  for(x in c("build","build_inferred")){
    if(!x %in% colnames(meta)){
      messager("Missing",paste0("'",x,"'"),"column.",
               "Cannot run check_build_matches.",
               v=verbose)
      return(meta)
    } 
  } 
  meta$build_matches <- toupper(
    stringr::str_split(meta$build,"/",
                       simplify = TRUE)[,2]
  )==meta$build_inferred
  #### Summarise ####
  messager(sum(meta$build_matches)," / ",nrow(meta),
          " inferred genome builds match the reported genome build.",
          v=verbose)
  return(meta)
}

assign_trait_groups <- function(meta,
                                traits,
                                verbose = TRUE){
  messager("Assigning each trait to a trait_group.",v=verbose)
  meta$trait_group <- stringr::str_extract(
    tolower(meta$trait),
    paste0("(",tolower(traits),")",collapse = "|"))
  meta[is.na(meta$trait_group),"trait_group"] <- "other"
  return(meta)
}

filter_traits <- function(meta,
                          traits = NULL,
                          consortia = NULL,
                          startswith_only = FALSE,
                          exclusion_terms = NULL,
                          group_var = "trait_group",
                          topn = Inf,
                          verbose = TRUE){
  ### Assign groups by substring search ####
  if(!is.null(traits)){
    meta <- assign_trait_groups(meta = meta, 
                                traits = traits, 
                                verbose = verbose)
  }
  #### Include only datasets that start with terms #####
  if(startswith_only){
    messager("Removing trait that don't start with search terms.",v=verbose)
    meta <- meta[grepl(paste0("^",traits,collapse = "|"),
                       meta$trait, 
                       ignore.case = TRUE),]
  }
  #### Exclude traits with certain terms ####
  if(!is.null(exclusion_terms)){
    messager("Excluding traits with the terms:",
             paste(exclusion_terms,collapse = ", "),v=verbose)
    meta <- meta[!grepl(paste0(exclusion_terms,collapse = "|"),
                       meta$trait, 
                       ignore.case = TRUE),]
  }
  #### Include only certai consortia ####
  if(!is.null(consortia)){
    messager("Only including datasets from:",
             paste(consortia,collapse = ", "),v=verbose)
    ## Catch UKB datasets that aren't labeled properly in "consortium"
    if("UK Biobank" %in% consortia){
      meta <- subset(meta, consortium %in% consortia | startsWith(id,"ukb"))
    }else {
      meta <- subset(meta, consortium %in% consortia)
    }
  }
  ##### Return only top N datasets ####
  if(!is.infinite(topn)){
    if(group_var %in% colnames(meta)){
      messager("Only including top",topn,"datasets per trait group.",v=verbose)
      meta <- meta %>%
        dplyr::group_by_at(.vars = group_var) %>%
        dplyr::top_n(n = topn, wt = N)
    }else {
      messager("group_var",paste0("'",group_var,"'"),
               "is not a column in the meta.",
               "topn filtering will not be used.",
               v=verbose)
    }
    
  } 
  messager("Returning metadata for",nrow(meta),"GWAS datasets.",v=verbose)
  return(data.table::data.table(meta))
}



#' List SNPs-to-genes mapping files
#' 
#' List paths to all SNPs-to-genes mapping files generated by
#' \link[MAGMA.Celltyping]{map_snps_to_genes}.
#' 
#' @param save_dir Directory to recursively search for matching files in.
#' @inheritParams base::list.files
#' 
#' @return Named list of paths.
#' 
#' @keywords internal
list_snps_to_genes_files <- function(save_dir,
                                     pattern = "*.genes.out$",
                                     verbose = TRUE){
  gene_files <- list.files(path = save_dir, 
                            pattern = pattern,
                            recursive = TRUE, 
                            full.names = TRUE)
  names(gene_files) <- basename(dirname(dirname(dirname(gene_files))))
  messager(length(gene_files),"files found.",v=verbose)
  return(gene_files)
}


#' Copy MAGMA gene mapping files to new folder
#' 
#' Copy SNPs-to-genes mapping files (\emph{.genes.out}) produced by 
#' \link[MAGMA.Celltyping]{map_snps_to_genes} to a new folder. 
#' Importantly, this function maintains the file naming and folder structure 
#' conventions necessary to run other downstream 
#' \pkg{MAGMA.Celltyping} functions.
#' 
#' @param gene_files Full paths to gene mapping mapping files.
#' @param save_dir Where to save the gene mapping files to.
#' @param overwrite Whether to overwrite gene mapping files that already
#'  exist in \code{save_dir} (Default: \code{FALSE})
#' 
#' @return Named character list
#' 
#' @keywords internal 
copy_snps_to_genes_files <- function(search_dir,
                                     pattern = "*.genes.out$|*.genes.raw$",
                                     save_dir = "MAGMA_Files",
                                     overwrite = FALSE){
  requireNamespace("parallel")
  gene_files <- list_snps_to_genes_files(save_dir = search_dir, 
                                         pattern = pattern)
  gene_files2 <- parallel::mclapply(gene_files, function(x){
    message_parallel(basename(x))
    new_file <- file.path(save_dir,
                          basename(dirname(x)),
                          basename(x))
    if(file.exists(new_file) && overwrite == FALSE){
      message_parallel("Skipping: File already exists.")
      return(new_file)
    } else {
      dir.create(dirname(new_file),
                 showWarnings = FALSE, recursive = TRUE)
      file.copy(x, new_file, overwrite = overwrite)
    }
    return(new_file)
  })
  #### Separate .raw and .out files ####
  genes.raw <- gene_files2[
    unlist(lapply(gene_files2,function(x){endsWith(x,suffix = ".genes.raw")}))]
  genes.out <- gene_files2[
    unlist(lapply(gene_files2,function(x){endsWith(x,suffix = ".genes.out")}))]
  return(list(genes.raw = genes.raw, 
              genes.out = genes.out))
}




list_sumstats <- function(save_dir = getwd(),
                          pattern = "*.tsv.gz$",
                          verbose = TRUE){ 
  munged_files <- list.files(path = save_dir,
                             pattern = pattern,
                             full.names = TRUE, 
                             recursive = TRUE)
  munged_files <- munged_files[basename(dirname(munged_files))!="logs"] 
  ids <- basename(dirname(munged_files))
  messager(length(munged_files),"files found.",v=verbose)
  return(setNames(munged_files,ids))
}


parse_logs <- function(save_dir = getwd(),
                       pattern = "MungeSumstats_log_msg.txt$",
                       verbose = TRUE){
  log_files <- list.files(path = save_dir,
                          pattern = pattern,
                          full.names = TRUE,
                          recursive = TRUE)
  messager("Parsing info from",length(log_files),"log files.",
           v=verbose)
  
  log_data <- lapply(seq_len(length(log_files)), function(i){
    f <- log_files[i]
    # messager(i,":",f)
    l <- readLines(f)
    data.table::data.table(
      #### Infer ID ####
      id = basename(dirname(dirname(f))),
      id_standard = parse_idStandard(l = l),
      #### Get metrics before format_sumstats ####
      rows_start = parse_report(l = l, entry = 1, line = 1),
      snps_start = parse_report(l = l, entry = 1, line = 2),
      sig_snps_start = parse_report(l = l, entry = 1, line = 3),
      chroms_start = parse_report(l = l, entry = 1, line = 4),
      #### Get metrics after format_sumstats ####
      rows_end = parse_report(l = l, entry = -1, line = 1),
      snps_end = parse_report(l = l, entry = -1, line = 2),
      sig_snps_end = parse_report(l = l, entry = -1, line = 3),
      chroms_end = parse_report(l = l, entry = -1, line = 4),
      #### Genome build ####
      build_inferred = parse_genome_build(l = l),
      #### Flipped ####
      snps_flipped = parse_flipped(l = l),
      #### Dropped ####
      snps_dropped_INFO = parse_dropped_INFO(l = l),
      snps_dropped_nonRef = parse_dropped_nonRef(l = l),
      snps_dropped_nonA1A2 = parse_dropped_nonA1A2(l = l),
      snps_dropped_duplicates = parse_dropped_duplicates(l = l),
      snps_dropped_chrom = parse_dropped_chrom(l = l),
      #### Path info ####
      log_path = f
    )  
  }) %>% data.table::rbindlist()
  return(log_data)
}


parse_report <- function(l, 
                         entry = 1,
                         line = 1){
  report_lines <- grep("Summary statistics report:",l)
  if(length(report_lines)<2 && entry!=1) return(NA)
  as.integer(
    gsub(",","",
         strsplit(
           trimws(
             l[report_lines+line][entry]
           ),
           " ")[[1]][2]
    )
  )
}
parse_genome_build <- function(l){
  line <- grep("Inferred genome build:",l,value = TRUE)[1]
  trimws(strsplit(line,":")[[1]][-1])
}
parse_dropped_nonRef <- function(l){
  line <- grep("are not on the reference genome",l,value = TRUE)[1]
  as.integer(trimws(gsub(",","",strsplit(line," SNPs")[[1]][1])))
}
parse_flipped <- function(l){
  line <- grep("where A1 doesn't match the reference genome",l,
               value = TRUE)[1]
  as.integer(trimws(gsub("There are|,","",strsplit(line," SNPs")[[1]][1])))
}
parse_dropped_INFO <- function(l){
  line <- grep("SNPs are below the INFO threshold of.",l,
               value = TRUE)[1]
  as.integer(trimws(gsub("There are|,","",strsplit(line," SNPs")[[1]][1])))
}
parse_dropped_nonA1A2 <- function(l){
  line <- grep("neither A1 nor A2 match the reference genome",l,
               value = TRUE)[1]
  as.integer(trimws(gsub("There are|,","",strsplit(line," SNPs")[[1]][1])))
}
parse_dropped_duplicates <- function(l){
  line <- grep("are duplicated in the sumstats file",l,
               value = TRUE)[1]
  as.integer(trimws(gsub(",","",strsplit(line," ")[[1]][1])))
}
parse_dropped_chrom <- function(l){
  line <- grep("are on chromosomes X, Y, MT and will be removed",l,
               value = TRUE)[1]
  as.integer(trimws(gsub(",","",strsplit(line," ")[[1]][1])))
}
parse_dropped_nonBiallelic <- function(l){
  line <- grep("SNPs are non-biallelic",l,
               value = TRUE)[1]
  as.integer(trimws(gsub(",","",strsplit(line," ")[[1]][1])))
}
parse_idStandard <- function(l){
  line <- grep("Parsing .*. data column",l,
               value = TRUE)[1]
  trimws(strsplit(line," ")[[1]][2], whitespace = "'|[ ]")
}


